from pyspark import SparkContext, SparkConf
import pyspark_cassandra

def myParser(line):
	b = line.split(",")
	return [b[0], b[-1]]


conf = SparkConf().setAppName("myBatch")
sc = SparkContext(conf=conf)
data = sc.textFile("hdfs://ec2-54-175-15-242.compute-1.amazonaws.com:9000/user/TestData/my-topic/20150920193500_0.txt")
formatted_data = data.map(lambda line: myParser(line)).reduceByKey(lambda a,b: (int(a)+int(b)))
res = formatted_data.collect()

for val in res:
	print val

print "Reading data from the HDFS"

from cassandra import ConsistencyLevel
from cassandra.cluster import Cluster
from cassandra.query import SimpleStatement

cluster = Cluster(['54.175.15.242'])
session = cluster.connect()
KEYSPACE = "keyspace_batch"

rows = session.execute("SELECT keyspace_name FROM system.schema_keyspaces")
if KEYSPACE in [row[0] for row in rows]:
	#log.debug("There is an existing keyspace...")
	#log.debug("setting keyspace...")
	session.set_keyspace(KEYSPACE)
else:
	session.execute("""
		CREATE KEYSPACE %s
		WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': '3' }
		""" % KEYSPACE)

	#log.debug("setting keyspace...")
	session.set_keyspace(KEYSPACE)

	#log.debug("creating table...")
	session.execute("""
		CREATE TABLE mytable (
			thekey text,
			col1 text,
			col2 text,
		PRIMARY KEY (thekey, col1)
		)
		""")

	session.execute("""
		CREATE TABLE mytable_RDD (
			thekey text,
			col1 text,
			PRIMARY KEY (thekey)
		)
		""")

query = SimpleStatement("""
    INSERT INTO mytable (thekey, col1, col2)
    VALUES (%(key)s, %(a)s, %(b)s)
    """, consistency_level=ConsistencyLevel.ONE)

#for i in range(10):
#	session.execute(query, dict(key="key%d" % i, a="xxxxxx%d" % i, b="xxxxxx"))

for val in res:
	i = 122330
	session.execute(query, dict(key="key%d" % i, a=val[0], b=str(val[1])))
	i += 1

print "Writing data to Cassandra"

print "Test for pyspark_cassandra"

#for i in range(10):
formatted_data.saveToCassandra(
	"keyspace_batch",
	"mytable_rdd",
)

#conf2 = SparkConf().setAppName("PySpark Cassandra Test").set("spark.cassandra.connection.host", "cas-1")
#sc2 = CassandraSparkContext(conf=conf)

#sc.cassandraTable("keyspace1", "table1").select("col-a", "col-b").where("key=?", "x").filter(lambda r: r["col-b"].contains("foo")).map(lambda r: (r["col-a"], 1).reduceByKey(lambda a, b: a + b).collect()


#rdd = sc.parallelize([{
#	"key": k,
#	"stamp": datetime.now(),
#	"val": random() * 10,
#	"tags": ["a", "b", "c"],
#	"options": {
#		"foo": "bar",
#		"baz": "qux",
#	}
#} for k in ["x", "y", "z"]])

#rdd.saveToCassandra(
#	"keyspace1",
#	"table1",
#	ttl=timedelta(hours=1),
#)


